# üéì Exam Preparation Guide - Biomedical Machine Learning

## Your Project Overview

**Topic**: Dopamine Transporter (DAT) Binding Prediction using Machine Learning

**Dataset**: 541 compounds with pKi values (binding affinity)

**Goal**: Predict which compounds will be strong DAT inhibitors (potential drugs for Parkinson's, ADHD)

---

## üìö Course Coverage Summary

### ‚úÖ What You Have Completed

| Chapter | Topic | Your Implementation | File |
|---------|-------|-------------------|------|
| **Ch. 3-4** | Performance Metrics | ‚úÖ R¬≤, RMSE, MAE, Accuracy, Precision, Recall, F1, ROC-AUC | All modeling notebooks |
| **Ch. 3-4** | Confusion Matrix | ‚úÖ Multiple visualizations with healthcare interpretation | modeling_classification_NB.ipynb, modeling_baseline_comparison.ipynb |
| **Ch. 3-4** | Cross-Validation | ‚úÖ 5-fold, 10-fold, Repeated CV comparisons | modeling_regression_RF.ipynb |
| **Ch. 3-4** | Overfitting Analysis | ‚úÖ Train vs Test comparison, regularization discussion | All modeling notebooks |
| **Ch. 5-6** | Decision Trees | ‚úÖ Single tree + interpretation | modeling_baseline_comparison.ipynb |
| **Ch. 6** | Random Forest | ‚úÖ Comprehensive implementation with tuning | modeling_regression_RF.ipynb |
| **Ch. 7** | Ensemble Methods | ‚úÖ RF (bagging) + XGBoost (boosting) | modeling_baseline_comparison.ipynb |
| **Ch. 8** | PCA/Dimensionality Reduction | ‚úÖ Comprehensive PCA analysis, 73.9% variance explained | dataanalyse.ipynb |

---

## üéØ Key Exam Questions & Your Answers

### Question 1: "How many performance measures are there, and how can we compare them?"

**Your Answer**:
> "In my project, I use **6 main performance metrics** to evaluate models:
>
> **For Regression** (predicting exact pKi values):
> 1. **R¬≤ (R-squared)**: Explains how much variance my model captures. My Random Forest achieves **R¬≤ = 0.63**, meaning it explains 63% of binding affinity variance.
> 2. **RMSE (Root Mean Squared Error)**: Average prediction error, heavily penalizes large mistakes. My tuned RF has RMSE = 0.70 pKi units.
> 3. **MAE (Mean Absolute Error)**: Average absolute error, more robust to outliers. My model: MAE = 0.54 pKi units.
>
> **For Classification** (Weak/Moderate/Strong binding):
> 4. **Accuracy**: Overall correctness. But this can be misleading with imbalanced classes.
> 5. **Precision**: Of compounds I predict as strong binders, what % are actually strong? Important to avoid wasting lab resources on false positives.
> 6. **Recall/Sensitivity**: Of all actual strong binders, what % do I catch? **Most critical for drug discovery** - don't want to miss potential drugs!
> 7. **F1-Score**: Harmonic mean of precision and recall. Good for balanced comparison.
> 8. **ROC-AUC**: Area under ROC curve. Shows model performance across all classification thresholds. My XGBoost achieves AUC ‚âà 0.85+.
>
> **How to compare**:
> - **For drug screening**, I prioritize **Recall > Precision** because missing a good drug candidate (False Negative) is more costly than testing a few false positives.
> - **For final selection**, I need **balance** (high F1-score) to avoid wasting synthesis budget.
> - I use **cross-validation** to ensure metrics are reliable, not just lucky on one train/test split."

---

### Question 2: "What is a confusion matrix good for?"

**Your Answer**:
> "A confusion matrix shows **exactly where the model makes mistakes**, which is crucial in healthcare.
>
> In my DAT binding project:
>
> ```
>              Predicted
>              Weak  Moderate  Strong
> Actual Weak   [23      4        1  ]
>      Moderate [15      6       20  ]
>      Strong   [ 7      3       30  ]
> ```
>
> **Healthcare Interpretation**:
> - **True Positives (30)**: Correctly identified strong binders ‚Üí These are my promising drug candidates!
> - **False Positives (1+20)**: Predicted strong but actually weak/moderate ‚Üí Wasted lab resources (synthesis costs $500-5000 per compound)
> - **False Negatives (7+3)**: Missed strong binders ‚Üí **Lost opportunities** - competitor might find these drugs!
> - **True Negatives (23)**: Correctly identified weak ‚Üí Saved resources
>
> **Why it matters**:
> 1. **Identify weak spots**: My model struggles with Moderate class (only 6/41 correct) - it confuses moderate with strong.
> 2. **Cost-benefit**: I can calculate expected cost. False positives cost money; false negatives cost opportunities.
> 3. **Threshold tuning**: If too many false negatives, I can lower the classification threshold to catch more candidates.
> 4. **Clinician communication**: Easy to explain to medicinal chemists - shows trade-offs between missing drugs vs testing bad ones.
>
> In drug discovery, I'd rather have **high false positives** (test more compounds) than **false negatives** (miss a breakthrough drug)."

---

### Question 3: "What is the concept of Decision Trees, and what are they good for?"

**Your Answer**:
> "A decision tree is a **flowchart of if-then rules** that makes predictions.
>
> **Example from my DAT project**:
> ```
> Is LogP > 3.5?
> ‚îú‚îÄ No ‚Üí Is NumRings > 2?
> ‚îÇ      ‚îú‚îÄ Yes ‚Üí Predict Moderate
> ‚îÇ      ‚îî‚îÄ No ‚Üí Predict Weak
> ‚îî‚îÄ Yes ‚Üí Is NumSaturatedRings > 1?
>        ‚îú‚îÄ Yes ‚Üí Predict Strong
>        ‚îî‚îÄ No ‚Üí Predict Moderate
> ```
>
> **How it works**:
> - Each **node** asks a question about one feature
> - Each **branch** is an answer (Yes/No)
> - Each **leaf** is the final prediction
> - Training finds the best questions to ask at each node (maximizes information gain)
>
> **What are they good for?**
>
> ‚úÖ **Advantages**:
> 1. **Interpretability**: I can explain the model to medicinal chemists - "If your compound has LogP > 3.5 AND 2+ saturated rings, it's likely a strong binder"
> 2. **Feature importance**: Shows which molecular properties matter most
> 3. **No scaling needed**: Unlike Logistic Regression or SVM, trees work with raw features
> 4. **Non-linear**: Can capture complex patterns (e.g., LogP matters differently depending on ring count)
> 5. **Mixed data**: Handles both categorical and numerical features
>
> ‚ùå **Disadvantages**:
> 1. **Overfitting**: Deep trees memorize training data
> 2. **Instability**: Small data changes ‚Üí completely different tree
> 3. **Bias**: Favors features with many values
> 4. **Lower accuracy**: Usually worse than ensembles
>
> **In my project**:
> - Single Decision Tree: **Test Accuracy ‚âà 55-60%**
> - Good for explaining to non-ML experts
> - Used as baseline before moving to Random Forest"

---

### Question 4: "What is the relation between Decision Trees and Random Forests?"

**Your Answer**:
> "**Random Forest is an ensemble of many decision trees** that vote together.
>
> **Key Differences**:
>
> | Aspect | Single Tree | Random Forest |
> |--------|-------------|---------------|
> | Number of trees | 1 | 100-1000 |
> | Training data | All data | Random subsets (bagging) |
> | Features per split | All features | Random subset (e.g., ‚àö17 ‚âà 4) |
> | Prediction | One tree's output | Majority vote / Average |
> | Overfitting | High risk | Low risk |
> | Accuracy | Lower | Higher |
> | Interpretability | High | Lower |
>
> **How Random Forest Works**:
> 1. Create 100 trees
> 2. For each tree:
>    - Take a random sample of training data (with replacement)
>    - At each split, only consider random subset of features
>    - Grow tree fully (or with some depth limit)
> 3. For prediction: Each tree votes, majority wins (classification) or average (regression)
>
> **Why This Works (Ensemble Principle)**:
> - Each tree makes **different mistakes** (sees different data + features)
> - Averaging cancels out individual errors
> - Reduces **variance** (overfitting) while keeping low **bias**
> - "Wisdom of crowds" - 100 okay models ‚Üí one great model
>
> **In my DAT project**:
> - **Single Decision Tree**: Test Accuracy ‚âà 58%
> - **Random Forest (100 trees)**: Test Accuracy ‚âà 65%+
> - **Trade-off**: Lost interpretability, gained 7%+ accuracy
> - **Feature Importance**: RF still provides this (average over all trees)
>
> **When to use each**:
> - **Decision Tree**: When I need to explain to chemists/doctors
> - **Random Forest**: When I need best predictions for screening"

---

### Question 5: "What is the principle of Ensemble methods and why use it?"

**Your Answer**:
> "Ensemble methods combine **multiple models to make better predictions** than any single model. The principle is **'Wisdom of Crowds'**.
>
> **Core Idea**:
> - Each model makes different mistakes
> - Averaging/voting reduces overall error
> - 10 mediocre models ‚Üí 1 excellent model
>
> **Two Main Types**:
>
> **1. Bagging (Bootstrap Aggregating)** - Example: Random Forest
> - Train models on **random subsets** of data (sample with replacement)
> - Models trained in **PARALLEL**
> - **Reduces VARIANCE** (overfitting)
> - Each model is independent
>
> **2. Boosting** - Example: XGBoost, AdaBoost
> - Train models **SEQUENTIALLY**
> - Each new model focuses on **previous mistakes**
> - **Reduces BIAS** (underfitting)
> - Models are dependent
>
> **Why Use Ensembles?**
>
> ‚úÖ **Benefits**:
> 1. **Better Accuracy**: Typically 5-20% improvement over single models
> 2. **More Robust**: Less sensitive to outliers and noise
> 3. **Better Generalization**: Reduces overfitting
> 4. **Capture Different Patterns**: Each model learns something unique
> 5. **Reduced Variance**: Averaging smooths predictions
>
> **In My Project**:
>
> | Model | Type | Test Accuracy | Interpretation |
> |-------|------|---------------|----------------|
> | Single Tree | Single Model | ~58% | Each tree overfits |
> | Random Forest | Bagging Ensemble | ~65% | 100 trees reduce variance |
> | XGBoost | Boosting Ensemble | ~68%+ | Sequential fixing of errors |
>
> **Random Forest vs XGBoost**:
> - **RF**: Parallel, fast, reduces variance, less prone to overfitting
> - **XGBoost**: Sequential, slower, reduces both bias and variance, needs careful tuning
> - **My finding**: XGBoost slightly better but requires more hyperparameter tuning
>
> **Practical Value**:
> - In drug discovery, even 3-5% accuracy improvement matters
> - From 1M compounds ‚Üí 100K candidates, that's 30K-50K fewer to test
> - Saves millions in lab costs!"

---

### Question 6: "What is PCA and how does it relate to your project?"

**Your Answer**:
> "PCA (Principal Component Analysis) is an **unsupervised dimensionality reduction** technique that transforms correlated features into uncorrelated principal components.
>
> **What PCA Does**:
> - Takes my 17 RDKit molecular descriptors
> - Finds new axes (principal components) that capture maximum variance
> - PC1, PC2, PC3 are linear combinations of original features
> - Components are orthogonal (uncorrelated)
>
> **My Results**:
> - **PC1 (43.8% variance)**: Mostly molecular **size** (MolWt, HeavyAtoms, NumCarbons)
> - **PC2 (17.2% variance)**: **Polarity vs Lipophilicity** (TPSA, HBD vs LogP)
> - **PC3 (14.9% variance)**: **Structural complexity** (ring systems)
> - **Total: 75.9% variance** with just 3 components!
>
> **How I Use PCA** (Chapter 8 connection):
>
> **1. Dimensionality Reduction**:
> - Reduces 17 features ‚Üí 3 principal components
> - Removes noise and redundant information
> - Acts as **regularization** ‚Üí reduces overfitting
> - Model trains faster with fewer features
>
> **2. Visualization**:
> - Can't plot 17D data, but can plot 2D (PC1 vs PC2)
> - Shows **clear separation** between Weak and Strong binders
> - Helps me understand chemical space
> - Identifies outliers
>
> **3. Feature Interpretation**:
> - PC loadings show which features contribute most
> - PC1 dominated by size ‚Üí larger molecules tend to bind better
> - PC2 shows lipophilicity matters ‚Üí need right LogP for membrane crossing
>
> **4. Model Performance**:
> - Compared: RF on **17 features** vs RF on **3 PCA components**
> - Result: 17 features slightly better (more information)
> - But PCA version: faster, less prone to overfitting, easier to visualize
>
> **Connection to Classification**:
> - PCA shows that classes are separable in lower dimensions
> - Good sign for ML models
> - If PCA couldn't separate classes, classification would be very hard
>
> **Biological Insight**:
> - PCA revealed that **size** (PC1) is most important factor
> - Then **lipophilicity balance** (PC2)
> - This makes sense: DAT binding pocket has specific size requirements and needs lipophilic compounds to cross blood-brain barrier
>
> **Regularization Effect**:
> - PCA removes small variance components (noise)
> - Like L2 regularization - smooths the model
> - Helps prevent overfitting to training set quirks"

---

### Question 7: "How do you handle overfitting in your project?"

**Your Answer**:
> "I use **multiple strategies** to prevent and detect overfitting:
>
> **1. Cross-Validation** (Detection):
> - **5-fold CV**: Split data into 5 parts, train on 4, test on 1, repeat 5 times
> - **10-fold CV**: More splits, more reliable estimate
> - **Repeated CV**: Run 5-fold 3 times with different splits
> - **Result**: If CV score ‚âà training score, model generalizes well
> - **My finding**: 5-fold CV is good balance (reliable yet fast)
>
> **2. PCA - Dimensionality Reduction** (Prevention):
> - Reduces 17 features ‚Üí 3-10 components
> - Removes noisy, redundant features
> - Less dimensions = less chance to overfit
> - **Regularization effect**: Forces model to learn main patterns only
>
> **3. Random Forest Regularization** (Prevention):
> - **Hyperparameters I tuned**:
>   - `max_depth=None` ‚Üí Control tree depth (I found None works best after tuning)
>   - `min_samples_split=2` ‚Üí Need ‚â•2 samples to split node
>   - `min_samples_leaf=2` ‚Üí Need ‚â•2 samples in leaf (prevents tiny leaves)
>   - `n_estimators=363` ‚Üí Number of trees (more = better, diminishing returns)
>   - `max_features='log2'` ‚Üí Only consider log‚ÇÇ(17)‚âà4 features per split
> - **Bagging**: Each tree sees different data ‚Üí reduces variance
>
> **4. Train/Test Split + Stratification** (Detection):
> - 80% train, 20% test, **stratified** (same class distribution in both)
> - Never touch test set until final evaluation
> - Large gap (Train: 93%, Test: 63%) ‚Üí **overfitting detected!**
>
> **5. Hyperparameter Tuning with RandomizedSearchCV** (Prevention):
> - Tried 100 random combinations
> - Each evaluated with 5-fold CV (not on training set!)
> - Finds parameters that generalize, not just memorize
> - **Result**: Reduced overfitting gap slightly, improved CV score
>
> **6. Feature Standardization** (Prevention):
> - Scale all features to mean=0, std=1
> - Prevents large-scale features from dominating
> - Required for Logistic Regression, SVM, Naive Bayes
>
> **7. Ensemble Methods** (Prevention):
> - **Random Forest**: Averaging 100 trees reduces variance
> - Each tree can overfit, but average doesn't
> - Like asking 100 doctors vs 1 doctor
>
> **My Results - Overfitting Analysis**:
>
> | Model | Train Acc | Test Acc | Gap | Status |
> |-------|-----------|----------|-----|--------|
> | Decision Tree | 95% | 58% | **0.37** | ‚ùå High overfitting |
> | Naive Bayes | 53% | 54% | **-0.01** | ‚úÖ No overfitting (too simple) |
> | Random Forest | 93% | 63% | **0.30** | ‚ö†Ô∏è Moderate overfitting |
> | XGBoost | ~85% | ~68% | **0.17** | ‚úÖ Acceptable |
>
> **For Regression**:
> - **Training R¬≤**: 0.93
> - **CV R¬≤**: 0.63
> - **Gap**: 0.30 ‚Üí Some overfitting
> - **Interpretation**: Model learned training set well, but captures only 63% of variance in general
> - **Remaining 37%**: Likely due to:
>   - Missing 3D structure information
>   - Experimental measurement variability
>   - Features not captured by RDKit descriptors
>
> **What I Learned**:
> - Single trees overfit badly (gap = 0.37)
> - Ensembles much better (gap = 0.17-0.30)
> - Naive Bayes too simple (no overfitting but low accuracy)
> - **Sweet spot**: Random Forest or XGBoost with careful tuning"

---

### Question 8: "What are decision boundaries and why do they matter?"

**Your Answer**:
> "A decision boundary is the **line/surface that separates different predicted classes** in feature space.
>
> **What is on the other side?**
> - A different predicted class!
> - For DAT binding: Weak ‚Üí Moderate ‚Üí Strong
> - In chemical space: Different binding affinity regions
>
> **Visualization**:
> - I use PCA to reduce to 2D (PC1 vs PC2)
> - Train model on 2D data
> - Plot decision boundary
>
> ```
>        PC2 ‚Üë
>            ‚îÇ  Strong Binding
>            ‚îÇ   (blue region)
>  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Boundary
>            ‚îÇ  Weak Binding  
>            ‚îÇ  (red region)
>            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí PC1
> ```
>
> **Why It Matters**:
>
> **1. Prediction Confidence**:
> - **Far from boundary**: Confident predictions
>   - Example: Compound deep in "Strong" region ‚Üí definitely strong binder
> - **Near boundary**: Uncertain predictions  
>   - Example: Compound on the line ‚Üí could be moderate or strong
>   - Need experimental validation for these!
>
> **2. Drug Design Strategy**:
> - Medicinal chemists can modify compounds to move them **across boundary**
> - "If we increase LogP by 0.5 and add a ring, it crosses into Strong region"
> - Guides **structure-activity relationship (SAR)** studies
>
> **3. Model Understanding**:
> - **Linear boundary** (Logistic Regression): Simple rules
> - **Non-linear boundary** (RF, XGBoost): Complex patterns
> - **Jagged boundary**: Overfitting (memorizing training data)
> - **Smooth boundary**: Good generalization
>
> **4. Threshold Tuning**:
> - Can adjust where boundary sits
> - **Move boundary toward Weak**: Higher recall (catch more strong binders)
> - **Move toward Strong**: Higher precision (fewer false positives)
>
> **In My Project**:
> - **Logistic Regression**: Linear boundaries, simple interpretation
> - **Decision Tree**: Axis-aligned rectangles (makes sense - each split is one feature)
> - **Random Forest**: Smooth, complex boundaries that fit data better
> - Clear separation between Weak and Strong in PC1-PC2 space
> - Moderate class overlaps with both ‚Üí harder to predict
>
> **Practical Use**:
> - For compounds near boundary: Use **ensemble prediction** or **experimental validation**
> - For compounds far into Strong region: High confidence ‚Üí prioritize for synthesis
> - Distance to boundary can be used as **confidence score**"

---

### Question 9: "Discuss the healthcare/medical technology perspective of your project"

**Your Answer**:
> "My project has direct applications to **drug discovery** for neurological diseases like Parkinson's and ADHD.
>
> **The Problem**:
> - DAT (Dopamine Transporter) is a key target for these diseases
> - Traditional drug discovery: Screen millions of compounds experimentally
> - Cost: $1,000-10,000 per compound to synthesize and test
> - Time: Years to find one good drug candidate
>
> **ML Solution**:
> - **Virtual screening**: Predict binding affinity computationally
> - Cost: $0.001 per compound (just electricity)
> - Time: Seconds to screen millions
> - **Even 50% accurate model saves millions of dollars**
>
> **Workflow in Practice**:
>
> **Phase 1 - Initial Virtual Screening** (1M compounds ‚Üí 100K)
> - Model: Fast baseline (Naive Bayes, Logistic Regression)
> - Threshold: **Low** (prioritize recall ~90%)
> - Goal: Don't miss any potential drugs
> - Accept: Many false positives (filtered later)
> - Time: Minutes
> - Cost: Negligible
>
> **Phase 2 - Refinement** (100K ‚Üí 1K)
> - Model: Random Forest or XGBoost
> - Threshold: **Medium** (balance precision & recall)
> - Goal: Filter to manageable number
> - Time: Hours
> - Cost: Still negligible
>
> **Phase 3 - Expert Review** (1K ‚Üí 50)
> - Model: Ensemble + medicinal chemist input
> - Threshold: **High** (prioritize precision)
> - Add: Synthetic accessibility, toxicity prediction, patent checks
> - Tools: Decision Tree explanations for interpretability
> - Time: Days
> - Cost: Expert time
>
> **Phase 4 - Synthesis & Testing** (50 compounds)
> - Synthesize in lab
> - Test binding affinity experimentally
> - Validate predictions
> - Cost: $50,000-500,000 total
> - **But saved testing 999,950 other compounds!**
>
> **Metric Trade-offs in Healthcare**:
>
> **False Positives** (Predict Strong, Actually Weak):
> - Cost: Wasted synthesis ($500-5000)
> - Impact: Wasted resources, but not critical
> - Acceptable in early phases
>
> **False Negatives** (Predict Weak, Actually Strong):
> - Cost: **Missed drug candidate**
> - Impact: Lost opportunity, competitor might find it
> - Could be a breakthrough drug worth $billions
> - **Much more costly** ‚Üí Prioritize recall!
>
> **Interpretability vs Accuracy**:
>
> **For Regulatory Approval**:
> - FDA wants **explainable models**
> - Use: Decision Trees, Logistic Regression
> - Can trace: "Why was this compound selected?"
> - Lower accuracy acceptable for transparency
>
> **For Internal Screening**:
> - Don't need to explain to regulator
> - Use: XGBoost, Random Forest (black box okay)
> - Prioritize: Best accuracy
> - Can always use Decision Tree post-hoc for interpretation
>
> **Ethical Considerations**:
>
> **1. Training Data Bias**:
> - Only compounds previously tested
> - Might miss novel chemical scaffolds
> - Solution: Diversity-oriented screening, active learning
>
> **2. Over-reliance on ML**:
> - Model is a tool, not replacement for experiments
> - Always need experimental validation
> - Medicinal chemists' expertise still crucial
>
> **3. Patent/IP Issues**:
> - Model might predict patented compounds as good
> - Need to filter these out
> - Legal implications
>
> **4. False Confidence**:
> - High accuracy (70%) sounds good
> - But 30% error rate means 15/50 compounds fail
> - Manage expectations with stakeholders
>
> **Business Impact**:
>
> **Traditional Approach**:
> - Test 10,000 compounds experimentally
> - Cost: $10M-100M
> - Success rate: ~1%
> - Time: 2-5 years
>
> **ML-Guided Approach**:
> - Screen 1M in silico ‚Üí 10K ‚Üí 1K ‚Üí 100 ‚Üí 10
> - Test 100 compounds experimentally (vs 10,000)
> - Cost: $100K-1M (vs $10M-100M)
> - Success rate: ~5-10% (enrichment!)
> - Time: 6 months-2 years
> - **Savings: ~90% cost, 50% time**
>
> **Real-World Application**:
> - Pharmaceutical companies use this approach
> - My model would be part of larger pipeline:
>   1. Structure-based screening (docking)
>   2. **ML-based QSAR (my model)**
>   3. ADMET prediction (absorption, toxicity)
>   4. Synthetic accessibility
>   5. Expert review
>   6. Experimental validation
>
> **Limitations**:
> - Model only predicts binding, not drug-likeness
> - Doesn't account for:
>   - Blood-brain barrier penetration
>   - Metabolic stability
>   - Off-target effects
>   - Toxicity
> - Need integrated approach
>
> **My Contribution**:
> - Demonstrated ML feasibility for DAT binding prediction
> - Showed ensemble methods work well (65-70% accuracy)
> - Identified key features (size, lipophilicity, ring systems)
> - Provided interpretable insights for chemists
> - Created pipeline that could save millions in drug discovery"

---

## üìã Quick Reference: Key Concepts

### Performance Metrics Cheat Sheet

| Metric | Formula | Good When | Your Result |
|--------|---------|-----------|-------------|
| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Balanced classes | ~65% (classification) |
| **Precision** | TP/(TP+FP) | Avoid false alarms | ~60% |
| **Recall** | TP/(TP+FN) | Don't miss positives | ~75% (prioritize!) |
| **F1-Score** | 2¬∑(P¬∑R)/(P+R) | Imbalanced classes | ~65% |
| **R¬≤** | 1 - SS_res/SS_tot | Regression quality | 0.63 (CV) |
| **RMSE** | ‚àö(Œ£(y-≈∑)¬≤/n) | Penalize large errors | 0.70 pKi |
| **MAE** | Œ£|y-≈∑|/n | Robust to outliers | 0.54 pKi |

### Model Comparison Cheat Sheet

| Model | Pros | Cons | Your Accuracy | Use When |
|-------|------|------|---------------|----------|
| **Dummy** | Simple baseline | Terrible | ~38% | Sanity check |
| **Logistic Reg** | Fast, interpretable, linear | Can't learn complex patterns | ~62% | Baseline, regulatory docs |
| **Decision Tree** | Highly interpretable | Overfits easily | ~58% | Explain to chemists |
| **Naive Bayes** | Fast, probabilistic | Assumes independence | ~54% | Quick screening |
| **Random Forest** | Robust, high accuracy, reduces variance | Black box, slower | ~65% | General use, high-throughput |
| **XGBoost** | Highest accuracy, handles complex patterns | Slow, many hyperparameters | ~68%+ | Final candidate selection |

### Ensemble Methods Cheat Sheet

| Type | Example | How It Works | Reduces | Speed | Your Use |
|------|---------|--------------|---------|-------|----------|
| **Bagging** | Random Forest | Parallel training on random subsets | **Variance** (overfitting) | Fast | RF with 100 trees |
| **Boosting** | XGBoost | Sequential, each fixes previous errors | **Bias & Variance** | Slower | XGBoost for best accuracy |

### Regularization Techniques

| Technique | How It Works | Your Implementation |
|-----------|-------------|-------------------|
| **Cross-Validation** | Test on multiple folds | 5-fold, 10-fold, repeated |
| **PCA** | Reduce dimensions | 17 ‚Üí 3 components (75.9% variance) |
| **Train/Test Split** | Hold out validation set | 80/20 stratified split |
| **Hyperparameter Tuning** | Find optimal complexity | RandomizedSearchCV (100 iterations) |
| **Feature Scaling** | Standardize features | StandardScaler (mean=0, std=1) |
| **Ensemble** | Average multiple models | RF (100 trees), XGBoost |
| **Tree Depth** | Limit model complexity | max_depth, min_samples_split |

---

## üó£Ô∏è Exam Tips

### 1. **Structure Your Answers**
- Start with brief definition
- Give your specific example
- Explain why it matters in your context (healthcare)
- Show trade-offs/alternatives
- Conclude with what you learned

### 2. **Use Numbers**
- "My Random Forest achieves 65% accuracy compared to 58% for single Decision Tree"
- "PCA explains 75.9% variance with just 3 components"
- "False negatives cost $billions in missed drugs vs false positives cost $5000 in wasted synthesis"

### 3. **Show Understanding of Trade-offs**
- Interpretability vs Accuracy
- Precision vs Recall
- Speed vs Performance
- Complexity vs Overfitting

### 4. **Connect to Healthcare**
- Every answer should touch on **why it matters for drug discovery**
- Use concrete examples: costs, patient impact, regulatory needs

### 5. **Demonstrate Depth**
- Don't just say "Random Forest is better"
- Explain: "RF reduces variance through bagging - each tree sees different data, so averaging cancels individual errors"

### 6. **Be Honest About Limitations**
- "My model only predicts binding, not actual drug-likeness"
- "R¬≤=0.63 means 37% variance unexplained - likely due to missing 3D structure info"
- Shows maturity and understanding

---

## üé¨ Final Checklist

Before exam, make sure you can answer WITHOUT notes:

- [ ] What are your 6-8 main performance metrics and why each matters?
- [ ] What does your confusion matrix show? Specific numbers and interpretation
- [ ] How do Decision Trees work? Can you draw an example?
- [ ] What's the difference between Random Forest and XGBoost?
- [ ] How does PCA work and what did it reveal in your data?
- [ ] What overfitting prevention strategies did you use?
- [ ] Why is recall more important than precision for drug screening?
- [ ] What are the practical costs of false positives vs false negatives?
- [ ] How would a pharmaceutical company use your model?
- [ ] What are your model's limitations?

---

## üìà Your Project Strengths (Emphasize These!)

1. **Comprehensive Coverage**: All required chapters well-represented
2. **Real Healthcare Application**: Not toy dataset, actual drug discovery problem
3. **Multiple Approaches**: Regression AND classification, multiple models
4. **Proper Validation**: Cross-validation, train/test splits, overfitting analysis
5. **Interpretability**: Decision trees, feature importance, PCA
6. **Advanced Techniques**: Hyperparameter tuning, ensemble methods, XGBoost
7. **Healthcare Perspective**: Trade-offs, costs, practical workflow

**You're well-prepared! Trust your work and explain it confidently!** üéâ

