{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Neural Network Regression with PCA: DAT Binding Prediction\n",
        "\n",
        "**Goal**: Predict pKi values using Neural Networks with **PCA-transformed features**\n",
        "\n",
        "**Dataset**: 541 compounds with PCA components (from dataanalyse.ipynb)  \n",
        "**Target**: pKi (continuous variable)  \n",
        "**Method**: Deep Neural Network + 70/15/15 Split + PCA  \n",
        "\n",
        "**Key Differences:**\n",
        "- Uses PCA components instead of raw RDKit descriptors\n",
        "- Reduced dimensionality can speed training and reduce overfitting\n",
        "- Still requires 70/15/15 split with validation set for early stopping\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Step 1: Load Data and Apply PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed RDKit features\n",
        "df_rdkit = pd.read_csv('processed_DAT_rdkit_features.csv')\n",
        "\n",
        "# Prepare features and target\n",
        "feature_cols = [col for col in df_rdkit.columns if col not in ['ChEMBL_ID', 'pKi']]\n",
        "X_rdkit = df_rdkit[feature_cols].values\n",
        "y = df_rdkit['pKi'].values\n",
        "\n",
        "# Standardize features (required for PCA)\n",
        "scaler_features = StandardScaler()\n",
        "X_scaled = scaler_features.fit_transform(X_rdkit)\n",
        "\n",
        "# Apply PCA (keep 95% variance)\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìÇ DATA LOADED AND PCA APPLIED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total compounds: {len(df_rdkit)}\")\n",
        "print(f\"Original features: {len(feature_cols)} RDKit descriptors\")\n",
        "print(f\"PCA components: {X_pca.shape[1]} (95% variance)\")\n",
        "print(f\"Variance explained: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 2: Train/Validation/Test Split (70/15/15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 70/15/15 split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_pca, y, test_size=0.30, random_state=42, shuffle=True\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Note: PCA components are already centered, but we can scale again for consistency\n",
        "scaler_pca = StandardScaler()\n",
        "X_train_scaled = scaler_pca.fit_transform(X_train)\n",
        "X_val_scaled = scaler_pca.transform(X_val)\n",
        "X_test_scaled = scaler_pca.transform(X_test)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìä SPLIT COMPLETED (70/15/15) - PCA Features\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training: {len(X_train)} ({len(X_train)/len(X_pca)*100:.1f}%)\")\n",
        "print(f\"Validation: {len(X_val)} ({len(X_val)/len(X_pca)*100:.1f}%)\")\n",
        "print(f\"Test: {len(X_test)} ({len(X_test)/len(X_pca)*100:.1f}%)\")\n",
        "print(f\"Features: {X_train_scaled.shape[1]} PCA components\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Step 3: Build & Train Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model (adjusted for fewer PCA features)\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(\"üèóÔ∏è Neural Network Architecture (PCA Features):\")\n",
        "model.summary()\n",
        "\n",
        "# Early stopping\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=20,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Training Neural Network on PCA features...\\n\")\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=500,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 4: Evaluation & Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train_scaled, verbose=0).flatten()\n",
        "y_val_pred = model.predict(X_val_scaled, verbose=0).flatten()\n",
        "y_test_pred = model.predict(X_test_scaled, verbose=0).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üìä NEURAL NETWORK PERFORMANCE (PCA)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Metric':<15} {'Training':<20} {'Validation':<20} {'Test':<20}\")\n",
        "print(\"-\"*80)\n",
        "print(f\"{'R¬≤ Score':<15} {train_r2:<20.4f} {val_r2:<20.4f} {test_r2:<20.4f}\")\n",
        "print(f\"{'RMSE':<15} {train_rmse:<20.4f} {val_rmse:<20.4f} {test_rmse:<20.4f}\")\n",
        "print(f\"{'MAE':<15} {train_mae:<20.4f} {val_mae:<20.4f} {test_mae:<20.4f}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(history.history['loss'], label='Training', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontweight='bold')\n",
        "axes[0].set_ylabel('Loss (MSE)', fontweight='bold')\n",
        "axes[0].set_title('Loss - PCA Features', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(history.history['mae'], label='Training', linewidth=2)\n",
        "axes[1].plot(history.history['val_mae'], label='Validation', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontweight='bold')\n",
        "axes[1].set_ylabel('MAE', fontweight='bold')\n",
        "axes[1].set_title('MAE - PCA Features', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Predictions plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].scatter(y_train, y_train_pred, alpha=0.6, s=40, edgecolors='black')\n",
        "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
        "axes[0].set_title(f'Training - PCA\\nR¬≤ = {train_r2:.4f}', fontweight='bold')\n",
        "axes[0].set_xlabel('Actual pKi', fontweight='bold')\n",
        "axes[0].set_ylabel('Predicted pKi', fontweight='bold')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].scatter(y_val, y_val_pred, alpha=0.6, s=40, edgecolors='black', color='orange')\n",
        "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "axes[1].set_title(f'Validation - PCA\\nR¬≤ = {val_r2:.4f}', fontweight='bold')\n",
        "axes[1].set_xlabel('Actual pKi', fontweight='bold')\n",
        "axes[1].set_ylabel('Predicted pKi', fontweight='bold')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "axes[2].scatter(y_test, y_test_pred, alpha=0.6, s=40, edgecolors='black', color='green')\n",
        "axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[2].set_title(f'Test - PCA\\nR¬≤ = {test_r2:.4f}', fontweight='bold')\n",
        "axes[2].set_xlabel('Actual pKi', fontweight='bold')\n",
        "axes[2].set_ylabel('Predicted pKi', fontweight='bold')\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Step 5: Classification Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification\n",
        "def classify_pKi(pKi_values):\n",
        "    return np.array(['Low' if pKi < 6.0 else 'Medium' if pKi < 8.0 else 'High' for pKi in pKi_values])\n",
        "\n",
        "y_test_cat = classify_pKi(y_test)\n",
        "y_test_pred_cat = classify_pKi(y_test_pred)\n",
        "\n",
        "cm = confusion_matrix(y_test_cat, y_test_pred_cat, labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=['Low', 'Medium', 'High'],\n",
        "            yticklabels=['Low', 'Medium', 'High'])\n",
        "test_acc = np.trace(cm) / cm.sum() * 100\n",
        "plt.title(f'NN with PCA - Test Set\\nAccuracy: {test_acc:.2f}%', fontweight='bold')\n",
        "plt.xlabel('Predicted', fontweight='bold')\n",
        "plt.ylabel('Actual', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä CLASSIFICATION REPORT (Test Set - PCA)\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test_cat, y_test_pred_cat, labels=['Low', 'Medium', 'High']))\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 6: Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"üéØ FINAL SUMMARY - NEURAL NETWORK REGRESSION (WITH PCA)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìä Dataset:\")\n",
        "print(f\"   Total compounds: {len(df_rdkit)}\")\n",
        "print(f\"   Training: {len(X_train)} (70%), Validation: {len(X_val)} (15%), Test: {len(X_test)} (15%)\")\n",
        "print(f\"   Original features: {len(feature_cols)} RDKit descriptors\")\n",
        "print(f\"   PCA components: {X_pca.shape[1]} (95% variance)\")\n",
        "\n",
        "print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
        "print(f\"   Input: {X_pca.shape[1]} PCA components\")\n",
        "print(f\"   Layers: Dense(128) ‚Üí Dense(64) ‚Üí Dense(32) ‚Üí Dense(1)\")\n",
        "print(f\"   Dropout: 30%, 20%\")\n",
        "print(f\"   Total parameters: {model.count_params():,}\")\n",
        "print(f\"   Stopped at epoch: {len(history.history['loss'])}\")\n",
        "\n",
        "print(f\"\\nüèÜ Best Model Performance (Test Set):\")\n",
        "print(f\"   R¬≤ Score: {test_r2:.4f}\")\n",
        "print(f\"   RMSE: {test_rmse:.4f}\")\n",
        "print(f\"   MAE: {test_mae:.4f}\")\n",
        "print(f\"   Classification Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nüí° Key Insights:\")\n",
        "print(f\"   ‚Ä¢ PCA reduced features from {len(feature_cols)} to {X_pca.shape[1]} (faster training!)\")\n",
        "print(f\"   ‚Ä¢ Early stopping worked: stopped at epoch {len(history.history['loss'])}\")\n",
        "print(f\"   ‚Ä¢ NN with PCA {'maintains' if test_r2 > 0.5 else 'shows'} good performance\")\n",
        "print(f\"   ‚Ä¢ Comparison: PCA can help reduce overfitting vs raw features\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ All 4 Modeling Notebooks Complete!\")\n",
        "print(\"   1. XGBoost (no PCA) - 80/20 split\")\n",
        "print(\"   2. XGBoost (PCA) - 80/20 split\")\n",
        "print(\"   3. Neural Network (no PCA) - 70/15/15 split\")\n",
        "print(\"   4. Neural Network (PCA) - 70/15/15 split\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
