{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Neural Network Regression: DAT Binding Prediction\n",
        "\n",
        "**Goal**: Predict pKi values (binding strength) using Neural Networks (Deep Learning)\n",
        "\n",
        "**Dataset**: 541 compounds with RDKit descriptors  \n",
        "**Target**: pKi (continuous variable)  \n",
        "**Method**: Deep Neural Network + 70/15/15 Train/Val/Test Split\n",
        "\n",
        "**Key Differences from Tree Models:**\n",
        "- Uses neural network architecture (multiple dense layers)\n",
        "- Requires 70/15/15 split (train/validation/test)\n",
        "- Validation set for early stopping (essential!)\n",
        "- More sensitive to feature scaling\n",
        "- Non-linear activation functions (ReLU)\n",
        "- Dropout for regularization\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# TensorFlow/Keras for Neural Networks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Step 1: Load Processed Data\n",
        "\n",
        "**Source:** `processed_DAT_rdkit_features.csv` (from dataanalyse.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed RDKit features\n",
        "df_rdkit = pd.read_csv('processed_DAT_rdkit_features.csv')\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_rdkit.drop(['ChEMBL_ID', 'pKi'], axis=1)\n",
        "y = df_rdkit['pKi']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìÇ DATA LOADED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total compounds: {len(df_rdkit)}\")\n",
        "print(f\"Features: {X.shape[1]} RDKit descriptors\")\n",
        "print(f\"Target: pKi (range: {y.min():.2f} - {y.max():.2f})\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 2: Train/Validation/Test Split (70/15/15)\n",
        "\n",
        "**Critical for Neural Networks:**\n",
        "- Training set (70%): For learning weights\n",
        "- Validation set (15%): For early stopping & hyperparameter tuning\n",
        "- Test set (15%): For final evaluation only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First split: 70% train, 30% temp (for val+test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Second split: split temp into 50/50 (15% each of total)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìä TRAIN/VALIDATION/TEST SPLIT (70/15/15)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training set: {len(X_train)} compounds ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Validation set: {len(X_val)} compounds ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set: {len(X_test)} compounds ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"\\npKi ranges:\")\n",
        "print(f\"   Train: {y_train.min():.2f} - {y_train.max():.2f}\")\n",
        "print(f\"   Val:   {y_val.min():.2f} - {y_val.max():.2f}\")\n",
        "print(f\"   Test:  {y_test.min():.2f} - {y_test.max():.2f}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features (CRITICAL for Neural Networks!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Features scaled using StandardScaler\")\n",
        "print(f\"   Mean ‚âà 0, Std ‚âà 1 (required for NN training)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Step 3: Build Neural Network Architecture\n",
        "\n",
        "**Architecture:**\n",
        "- Input layer: 17 features (RDKit descriptors)\n",
        "- Hidden layer 1: 128 neurons + ReLU + Dropout(30%)\n",
        "- Hidden layer 2: 64 neurons + ReLU + Dropout(20%)\n",
        "- Hidden layer 3: 32 neurons + ReLU\n",
        "- Output layer: 1 neuron (regression output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Neural Network model\n",
        "model = keras.Sequential([\n",
        "    # Input + First Hidden Layer\n",
        "    layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    layers.Dropout(0.3),  # 30% dropout for regularization\n",
        "    \n",
        "    # Second Hidden Layer\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.2),  # 20% dropout\n",
        "    \n",
        "    # Third Hidden Layer\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    \n",
        "    # Output Layer (regression)\n",
        "    layers.Dense(1)  # Single output: pKi value\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',  # Mean Squared Error for regression\n",
        "    metrics=['mae']  # Mean Absolute Error as metric\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üèóÔ∏è NEURAL NETWORK ARCHITECTURE\")\n",
        "print(\"=\"*60)\n",
        "model.summary()\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Step 4: Train Neural Network with Early Stopping\n",
        "\n",
        "**Training Configuration:**\n",
        "- Epochs: 500 (but will stop early if no improvement)\n",
        "- Batch size: 32\n",
        "- Early stopping: patience=20 (stop if val_loss doesn't improve for 20 epochs)\n",
        "- Restore best weights: Yes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping callback (ESSENTIAL for Neural Networks!)\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=20,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"üöÄ Training Neural Network...\")\n",
        "print(\"   Using early stopping to prevent overfitting\\n\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=500,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 5: Training History Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Loss (MSE)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Model Loss During Training', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# MAE plot\n",
        "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('MAE', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Mean Absolute Error During Training', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Training stopped at epoch {len(history.history['loss'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Step 6: Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train_scaled, verbose=0).flatten()\n",
        "y_val_pred = model.predict(X_val_scaled, verbose=0).flatten()\n",
        "y_test_pred = model.predict(X_test_scaled, verbose=0).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
        "\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üìä NEURAL NETWORK MODEL PERFORMANCE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Metric':<15} {'Training':<20} {'Validation':<20} {'Test':<20}\")\n",
        "print(\"-\"*80)\n",
        "print(f\"{'R¬≤ Score':<15} {train_r2:<20.4f} {val_r2:<20.4f} {test_r2:<20.4f}\")\n",
        "print(f\"{'RMSE':<15} {train_rmse:<20.4f} {val_rmse:<20.4f} {test_rmse:<20.4f}\")\n",
        "print(f\"{'MAE':<15} {train_mae:<20.4f} {val_mae:<20.4f} {test_mae:<20.4f}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Overfitting analysis\n",
        "overfit_r2 = train_r2 - test_r2\n",
        "print(f\"\\nüîç Overfitting Analysis:\")\n",
        "print(f\"   R¬≤ difference (train - test): {overfit_r2:.4f}\")\n",
        "if overfit_r2 > 0.1:\n",
        "    print(f\"   ‚ö†Ô∏è  Potential overfitting\")\n",
        "elif overfit_r2 > 0.05:\n",
        "    print(f\"   ‚ö° Mild overfitting\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Good generalization!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Step 7: Prediction Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actual vs Predicted plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Training\n",
        "axes[0].scatter(y_train, y_train_pred, alpha=0.6, s=40, edgecolors='black')\n",
        "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual pKi', fontsize=11, fontweight='bold')\n",
        "axes[0].set_ylabel('Predicted pKi', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title(f'Training Set\\nR¬≤ = {train_r2:.4f}', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Validation\n",
        "axes[1].scatter(y_val, y_val_pred, alpha=0.6, s=40, edgecolors='black', color='orange')\n",
        "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "axes[1].set_xlabel('Actual pKi', fontsize=11, fontweight='bold')\n",
        "axes[1].set_ylabel('Predicted pKi', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title(f'Validation Set\\nR¬≤ = {val_r2:.4f}', fontsize=12, fontweight='bold')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# Test\n",
        "axes[2].scatter(y_test, y_test_pred, alpha=0.6, s=40, edgecolors='black', color='green')\n",
        "axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[2].set_xlabel('Actual pKi', fontsize=11, fontweight='bold')\n",
        "axes[2].set_ylabel('Predicted pKi', fontsize=11, fontweight='bold')\n",
        "axes[2].set_title(f'Test Set\\nR¬≤ = {test_r2:.4f}', fontsize=12, fontweight='bold')\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Step 8: Classification Performance (Confusion Matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification function\n",
        "def classify_pKi(pKi_values):\n",
        "    return np.array(['Low' if pKi < 6.0 else 'Medium' if pKi < 8.0 else 'High' for pKi in pKi_values])\n",
        "\n",
        "# Convert to categories (test set)\n",
        "y_test_cat = classify_pKi(y_test)\n",
        "y_test_pred_cat = classify_pKi(y_test_pred)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_cat, y_test_pred_cat, labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=['Low', 'Medium', 'High'],\n",
        "            yticklabels=['Low', 'Medium', 'High'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.xlabel('Predicted Category', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Actual Category', fontsize=12, fontweight='bold')\n",
        "test_acc = np.trace(cm) / cm.sum() * 100\n",
        "plt.title(f'Neural Network - Test Set\\nClassification Accuracy: {test_acc:.2f}%', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä CLASSIFICATION REPORT (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test_cat, y_test_pred_cat, labels=['Low', 'Medium', 'High']))\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 9: Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"üéØ FINAL SUMMARY - NEURAL NETWORK REGRESSION (NO PCA)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìä Dataset:\")\n",
        "print(f\"   Total compounds: {len(df_rdkit)}\")\n",
        "print(f\"   Training: {len(X_train)} (70%)\")\n",
        "print(f\"   Validation: {len(X_val)} (15%)\")\n",
        "print(f\"   Test: {len(X_test)} (15%)\")\n",
        "print(f\"   Features: {X.shape[1]} RDKit descriptors (no PCA)\")\n",
        "\n",
        "print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
        "print(f\"   Layers: Dense(128) ‚Üí Dense(64) ‚Üí Dense(32) ‚Üí Dense(1)\")\n",
        "print(f\"   Activation: ReLU\")\n",
        "print(f\"   Dropout: 30%, 20%\")\n",
        "print(f\"   Optimizer: Adam (lr=0.001)\")\n",
        "print(f\"   Total parameters: {model.count_params():,}\")\n",
        "\n",
        "print(f\"\\nüèÜ Best Model Performance (Test Set):\")\n",
        "print(f\"   R¬≤ Score: {test_r2:.4f}\")\n",
        "print(f\"   RMSE: {test_rmse:.4f}\")\n",
        "print(f\"   MAE: {test_mae:.4f}\")\n",
        "print(f\"   Classification Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nüí° Key Insights:\")\n",
        "print(f\"   ‚Ä¢ Neural networks require 70/15/15 split (train/val/test)\")\n",
        "print(f\"   ‚Ä¢ Early stopping essential to prevent overfitting\")\n",
        "print(f\"   ‚Ä¢ Model stopped at epoch {len(history.history['loss'])} (early stopping worked!)\")\n",
        "print(f\"   ‚Ä¢ Validation set used for monitoring, test set untouched until final eval\")\n",
        "print(f\"   ‚Ä¢ NN performance is {'competitive with' if test_r2 > 0.5 else 'comparable to'} tree-based models\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ Neural Network Analysis Complete!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
