{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ XGBoost Regression with PCA: DAT Binding Prediction\n",
        "\n",
        "**Goal**: Predict pKi values (binding strength) using XGBoost with **PCA-transformed features**\n",
        "\n",
        "**Dataset**: 541 compounds with PCA components (from dataanalyse.ipynb)  \n",
        "**Target**: pKi (continuous variable)  \n",
        "**Method**: XGBoost Regression + 80/20 Train/Test Split + PCA  \n",
        "**Key Difference**: Uses **PCA components** instead of raw RDKit descriptors\n",
        "\n",
        "**Why PCA?**\n",
        "- Reduces dimensionality (17 features â†’ fewer components)\n",
        "- Removes multicollinearity between features\n",
        "- Can improve model generalization\n",
        "- Faster training with fewer features\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‚ Step 1: Load Processed Data and Apply PCA\n",
        "\n",
        "**Source:** `processed_DAT_rdkit_features.csv` (from dataanalyse.ipynb)\n",
        "\n",
        "**Process:**\n",
        "1. Load RDKit features (same as XGBoost without PCA)\n",
        "2. Standardize features (required for PCA)\n",
        "3. Apply PCA transformation\n",
        "4. Split into train/test (80/20)\n",
        "5. Use PCA components as features for XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed RDKit features from data analysis\n",
        "df_rdkit = pd.read_csv('processed_DAT_rdkit_features.csv')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ“‚ LOADED PROCESSED DATA FROM ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ… Dataset: {len(df_rdkit)} compounds\")\n",
        "print(f\"âœ… Features: {len(df_rdkit.columns)-2} RDKit descriptors\")\n",
        "print(f\"âœ… Source: dataanalyse.ipynb (same features as PCA!)\")\n",
        "print(f\"\\nðŸ“Š pKi distribution:\")\n",
        "print(f\"   Min: {df_rdkit['pKi'].min():.2f}\")\n",
        "print(f\"   Max: {df_rdkit['pKi'].max():.2f}\")\n",
        "print(f\"   Mean: {df_rdkit['pKi'].mean():.2f}\")\n",
        "print(f\"   Median: {df_rdkit['pKi'].median():.2f}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”„ Step 2: Apply PCA Transformation\n",
        "\n",
        "**Same process as in dataanalyse.ipynb:**\n",
        "1. Extract RDKit descriptors\n",
        "2. Standardize features (critical for PCA)\n",
        "3. Fit PCA to find principal components (keep 95% variance)\n",
        "4. Transform data to PCA space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features (X) and target (y)\n",
        "feature_cols = [col for col in df_rdkit.columns if col not in ['ChEMBL_ID', 'pKi']]\n",
        "X_rdkit = df_rdkit[feature_cols].values\n",
        "y = df_rdkit['pKi'].values\n",
        "\n",
        "# Standardize features (REQUIRED for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_rdkit)\n",
        "\n",
        "# Apply PCA (keep 95% variance - same as dataanalyse.ipynb)\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ”„ PCA TRANSFORMATION APPLIED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original features: {X_rdkit.shape[1]} RDKit descriptors\")\n",
        "print(f\"PCA components: {X_pca.shape[1]} (explaining 95% variance)\")\n",
        "print(f\"Variance explained: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
        "print(f\"\\nVariance per component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_, 1):\n",
        "    print(f\"   PC{i}: {var*100:.2f}%\")\n",
        "print(f\"\\nCumulative variance:\")\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "for i, var in enumerate(cumsum, 1):\n",
        "    print(f\"   PC1-PC{i}: {var*100:.2f}%\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Step 3: Train/Test Split (80/20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 80/20 Train-Test Split on PCA-transformed data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ“Š TRAIN-TEST SPLIT (80/20) - PCA Features\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training set: {X_train.shape[0]} compounds ({X_train.shape[0]/len(X_pca)*100:.1f}%)\")\n",
        "print(f\"Test set: {X_test.shape[0]} compounds ({X_test.shape[0]/len(X_pca)*100:.1f}%)\")\n",
        "print(f\"Features: {X_train.shape[1]} PCA components\")\n",
        "print(f\"\\nTraining pKi range: {y_train.min():.2f} - {y_train.max():.2f}\")\n",
        "print(f\"Test pKi range: {y_test.min():.2f} - {y_test.max():.2f}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Step 4: Train Baseline XGBoost Model on PCA Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize baseline XGBoost model\n",
        "xgb_baseline = XGBRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Training Baseline XGBoost Model (PCA Features)...\")\n",
        "\n",
        "# Train with early stopping\n",
        "xgb_baseline.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = xgb_baseline.predict(X_train)\n",
        "y_test_pred = xgb_baseline.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“Š BASELINE XGBOOST MODEL PERFORMANCE (PCA)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Metric':<20} {'Training Set':<20} {'Test Set':<20}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'RÂ² Score':<20} {train_r2:<20.4f} {test_r2:<20.4f}\")\n",
        "print(f\"{'RMSE':<20} {train_rmse:<20.4f} {test_rmse:<20.4f}\")\n",
        "print(f\"{'MAE':<20} {train_mae:<20.4f} {test_mae:<20.4f}\")\n",
        "print(\"-\"*70)\n",
        "overfit_r2 = train_r2 - test_r2\n",
        "print(f\"\\nðŸ” Overfitting: {overfit_r2:.4f} ({'Good' if overfit_r2 < 0.05 else 'Mild' if overfit_r2 < 0.1 else 'High'})\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ˆ Step 5: Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Actual vs Predicted plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "axes[0].scatter(y_train, y_train_pred, alpha=0.6, edgecolors='black', s=50)\n",
        "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
        "             'r--', lw=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Actual pKi', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Predicted pKi', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title(f'Training Set - PCA\\nRÂ² = {train_r2:.4f}', fontsize=13, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].scatter(y_test, y_test_pred, alpha=0.6, edgecolors='black', s=50, color='orange')\n",
        "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
        "             'r--', lw=2, label='Perfect Prediction')\n",
        "axes[1].set_xlabel('Actual pKi', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Predicted pKi', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title(f'Test Set - PCA\\nRÂ² = {test_r2:.4f}', fontsize=13, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Step 6: Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter distributions\n",
        "param_distributions = {\n",
        "    'n_estimators': randint(100, 1000),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'min_child_weight': randint(1, 7),\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    'colsample_bytree': uniform(0.6, 0.4),\n",
        "    'gamma': uniform(0, 0.5),\n",
        "    'reg_alpha': uniform(0, 1),\n",
        "    'reg_lambda': uniform(0, 2)\n",
        "}\n",
        "\n",
        "xgb_tuning = XGBRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_tuning,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=100,\n",
        "    scoring='r2',\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"ðŸ” Starting hyperparameter tuning (100 iterations, 5-fold CV)...\\n\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "best_xgb_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate tuned model\n",
        "y_train_pred_tuned = best_xgb_model.predict(X_train)\n",
        "y_test_pred_tuned = best_xgb_model.predict(X_test)\n",
        "\n",
        "train_r2_tuned = r2_score(y_train, y_train_pred_tuned)\n",
        "test_r2_tuned = r2_score(y_test, y_test_pred_tuned)\n",
        "test_rmse_tuned = np.sqrt(mean_squared_error(y_test, y_test_pred_tuned))\n",
        "test_mae_tuned = mean_absolute_error(y_test, y_test_pred_tuned)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… TUNED MODEL PERFORMANCE (PCA)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Test RÂ²: {test_r2_tuned:.4f} (baseline: {test_r2:.4f})\")\n",
        "print(f\"Test RMSE: {test_rmse_tuned:.4f} (baseline: {test_rmse:.4f})\")\n",
        "print(f\"Test MAE: {test_mae_tuned:.4f} (baseline: {test_mae:.4f})\")\n",
        "print(f\"\\nImprovement: {((test_r2_tuned - test_r2) / abs(test_r2) * 100):+.2f}%\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Step 7: Classification Performance (Confusion Matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification function\n",
        "def classify_pKi(pKi_values):\n",
        "    return np.array(['Low' if pKi < 6.0 else 'Medium' if pKi < 8.0 else 'High' for pKi in pKi_values])\n",
        "\n",
        "# Convert to categories\n",
        "y_test_cat = classify_pKi(y_test)\n",
        "y_test_pred_baseline_cat = classify_pKi(y_test_pred)\n",
        "y_test_pred_tuned_cat = classify_pKi(y_test_pred_tuned)\n",
        "\n",
        "# Confusion matrices\n",
        "cm_baseline = confusion_matrix(y_test_cat, y_test_pred_baseline_cat, labels=['Low', 'Medium', 'High'])\n",
        "cm_tuned = confusion_matrix(y_test_cat, y_test_pred_tuned_cat, labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "# Side-by-side visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])\n",
        "axes[0].set_title(f'Baseline - PCA\\nAccuracy: {np.trace(cm_baseline)/cm_baseline.sum()*100:.2f}%', fontweight='bold')\n",
        "axes[0].set_xlabel('Predicted', fontweight='bold')\n",
        "axes[0].set_ylabel('Actual', fontweight='bold')\n",
        "\n",
        "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
        "            xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])\n",
        "tuned_acc = np.trace(cm_tuned)/cm_tuned.sum()*100\n",
        "axes[1].set_title(f'Tuned - PCA\\nAccuracy: {tuned_acc:.2f}%', fontweight='bold')\n",
        "axes[1].set_xlabel('Predicted', fontweight='bold')\n",
        "axes[1].set_ylabel('Actual', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š CLASSIFICATION REPORT (Tuned Model - PCA)\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test_cat, y_test_pred_tuned_cat, labels=['Low', 'Medium', 'High']))\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 8: Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ðŸŽ¯ FINAL SUMMARY - XGBOOST REGRESSION (WITH PCA)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nðŸ“Š Dataset:\")\n",
        "print(f\"   Total compounds: {len(df_rdkit)}\")\n",
        "print(f\"   Training: {len(X_train)} (80%), Test: {len(X_test)} (20%)\")\n",
        "print(f\"   Original features: {len(feature_cols)} RDKit descriptors\")\n",
        "print(f\"   PCA components: {X_pca.shape[1]} (95% variance)\")\n",
        "\n",
        "print(f\"\\nðŸ† Best Model Performance (Test Set):\")\n",
        "print(f\"   RÂ² Score: {test_r2_tuned:.4f}\")\n",
        "print(f\"   RMSE: {test_rmse_tuned:.4f}\")\n",
        "print(f\"   MAE: {test_mae_tuned:.4f}\")\n",
        "print(f\"   Classification Accuracy: {tuned_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Key Insights:\")\n",
        "print(f\"   â€¢ PCA reduced dimensionality from {len(feature_cols)} to {X_pca.shape[1]} features\")\n",
        "print(f\"   â€¢ XGBoost with PCA provides {'improved' if test_r2_tuned > 0.5 else 'reasonable'} generalization\")\n",
        "print(f\"   â€¢ 80/20 split allows fair comparison with non-PCA and neural network models\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… XGBoost with PCA Analysis Complete!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
